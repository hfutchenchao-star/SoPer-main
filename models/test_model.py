import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Union
from transformers.cache_utils import Cache
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import Qwen2ForCausalLM, PretrainedConfig, AutoTokenizer
from models.gcn_gib import SocialGCN_GBSR
from torch_geometric.data import Data
from tqdm import tqdm

EMBED_SIZE = 1024
HIDDEN_SIZE = 1024
MAX_FRIEND_LEN = 8
max_length = 3572
max_friend_len = 8
training = False


class SparseAutoEncoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size, dtype=torch.bfloat16),
            nn.GELU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size, input_size, dtype=torch.bfloat16),
            nn.GELU(),
        )
        self.rho = 0.05
        self.rho_hat = None

    def forward(self, x):
        x = x.to(next(self.encoder.parameters()).dtype)
        z = self.encoder(x)
        self.rho_hat = z.mean(dim=1, keepdim=True)  # ✅ keepdim=True 避免维度问题
        x_recon = self.decoder(z)
        return z, x_recon

    def sae_loss(self, x, x_recon):
        eps = 1e-6
        # ✅ 修复：确保 rho_hat 维度正确
        rho_hat = self.rho_hat.mean()  # 标量
        rho_hat = torch.clamp(rho_hat, eps, 1 - eps)

        recon_loss = F.smooth_l1_loss(x, x_recon)

        # ✅ 修复：防止 log(0)
        rho = torch.clamp(torch.tensor(self.rho, device=x.device), eps, 1 - eps)
        kl_div = rho * torch.log(rho / rho_hat) + \
                 (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))

        # ✅ 检查 NaN
        if torch.isnan(kl_div):
            kl_div = torch.tensor(0.0, device=x.device)

        return recon_loss, kl_div


class DEPModel(Qwen2ForCausalLM):
    def __init__(self, config):
        super().__init__(config)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.sae = SparseAutoEncoder(EMBED_SIZE, HIDDEN_SIZE)
        self.sae.to_empty(device=device)

        self.gcn = SocialGCN_GBSR(
            in_channels=1025,      #  输入特征维度 = 度(1) + embedding(1024)
            hidden_channels=512,    #  隐层维度，可调
            out_channels=512,       #  输出维度，用于传给上层模型
            num_layers=3,          #  3层图卷积是一个经验值
            dropout=0.1,           #  GCN 正则化
            gib_sigma=0.5,         #  GIB 随机噪声标准差
            projector_dim=1024,     #  投影头维度（如果你做 contrastive / mutual-info）
            temperature=0.2,       #  对比学习温度
        )
        self.gcn.to_empty(device=device)

        self.align_mlp_user = nn.Sequential(
            nn.Linear(HIDDEN_SIZE, config.hidden_size, dtype=torch.float32),
            nn.GELU(),
            nn.Linear(config.hidden_size, config.hidden_size, dtype=torch.float32),
        )
        self.align_mlp_user.to_empty(device=device)

        self.align_mlp_friend = nn.Sequential(
            nn.Linear(HIDDEN_SIZE, config.hidden_size, dtype=torch.float32),
            nn.GELU(),
            nn.Linear(config.hidden_size, config.hidden_size, dtype=torch.float32),
        )
        self.align_mlp_friend.to_empty(device=device)

        self.align_mlp_graph = nn.Sequential(
            nn.Linear(1024, config.hidden_size, dtype=torch.float32),
            nn.GELU(),
            nn.Linear(config.hidden_size, config.hidden_size, dtype=torch.float32),
        )
        self.align_mlp_graph.to_empty(device=device)  
    def forward(self, *args, **kwargs):
    # 不加任何自定义逻辑，直接走原始 LLM forward
        return super().forward(*args, **kwargs)
    @torch.no_grad()
    def generate_text(
            self,
            input_ids: Optional[torch.LongTensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            past_key_values: Optional[Cache] = None,
            inputs_embeds: Optional[torch.FloatTensor] = None,
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            cache_position: Optional[torch.LongTensor] = None,
            logits_to_keep: Union[int, torch.Tensor] = 0,
            embeddings: Optional[torch.FloatTensor] = None,
            graph_data: Optional[Union[list, dict, Data]] = None,
            inp_str: Optional[Union[str, list]] = None,
            out_str: Optional[Union[str, list]] = None,
            center_user_id: Optional[Union[str, list]] = None,
            training: bool = True,
            **kwargs,
    ) -> Union[Tuple, CausalLMOutputWithPast]:

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )

        # ✅ 关键修复：确保 embeddings 存在且在正确设备上
        if embeddings is None:
            raise ValueError("❌ embeddings is None! 必须在 collate_fn 中提供。")

        if isinstance(embeddings, list):
            embeddings = torch.tensor(embeddings, dtype=torch.float32)
        embeddings = embeddings.to(self.device)

        def _to_pyg_data(g):
            if g is None:
                print("图数据不存在")
                return None, None, None
            if isinstance(g, Data):
                center_local = int(g.center_local.item()) if hasattr(g, "center_local") else 0
                node_names = getattr(g, "node_names", None)
                return g, center_local, node_names
            x = torch.tensor(g["x"], dtype=torch.bfloat16) if isinstance(g["x"], list) else g["x"]
            ei = torch.tensor(g["edge_index"], dtype=torch.long) if isinstance(g["edge_index"], list) else g[
                "edge_index"]
            data = Data(x=x, edge_index=ei)
            data.center_uid = g.get("center_uid", None)
            cl = g.get("center_local", 0)
            data.center_local = torch.tensor(cl, dtype=torch.long)
            node_names = g.get("node_names", None)
            return data, int(cl), node_names

        edge_mask = None
        graph_emb = None
        hsic_loss_total = 0.0
        num_graphs = 0

        if graph_data is None:
            raise ValueError("❌ graph_data is None!")

        # ✅ 处理批量图
        if isinstance(graph_data, list):
            graph_emb_list = []
            mask_list = []

            for g in graph_data:
                g_data, center_local, node_names = _to_pyg_data(g)
                if g_data is None:
                    graph_emb_list.append(torch.zeros(256, device=self.device, dtype=torch.bfloat16))
                    mask_list.append(torch.zeros(max_friend_len, device=self.device, dtype=torch.float32))
                    continue

                g_res = self.gcn.forward(
                    g_data.x.to(self.device),
                    g_data.edge_index.to(self.device),
                    batch_idx=None,
                    return_all=True
                )

                emb_original = g_res["emb_before"]
                emb_masked = g_res["emb_after"]
                edge_mask_raw = g_res["edge_mask"]  # ✅ 修改点

                center_emb_ori = emb_original[center_local].unsqueeze(0)
                center_emb_mask = emb_masked[center_local].unsqueeze(0)
                hsic_loss = self.gcn.compute_hsic_loss(center_emb_ori, center_emb_mask)
                hsic_loss_total += hsic_loss
                num_graphs += 1

                graph_emb_proj = self.gcn.llm_projector(center_emb_mask)
                graph_emb_list.append(graph_emb_proj.squeeze(0))

                ei = g_data.edge_index.to(self.device)
                src, dst = ei[0], ei[1]
                num_nodes = g_data.x.size(0)
                friends_order = [i for i in range(num_nodes) if i != center_local]

                weights_in_order = []
                for i in friends_order:
                    idx_0i = torch.where((src == center_local) & (dst == i))[0]
                    idx_i0 = torch.where((src == i) & (dst == center_local))[0]
                    ws = []
                    if idx_0i.numel() > 0:
                        ws.extend(edge_mask_raw[idx_0i].tolist())
                    if idx_i0.numel() > 0:
                        ws.extend(edge_mask_raw[idx_i0].tolist())
                    avg_w = float(sum(ws) / len(ws)) if len(ws) > 0 else 0.0
                    weights_in_order.append(avg_w)

                # ✅ padding
                if len(weights_in_order) == 0:
                    pad_weights = [0.0] * max_friend_len
                else:
                    pad_weights = (weights_in_order * ((max_friend_len // len(weights_in_order)) + 1))[:max_friend_len]

                mask_list.append(torch.tensor(pad_weights, dtype=torch.float32, device=self.device))

            graph_emb = torch.stack(graph_emb_list, dim=0)
            edge_mask = torch.stack(mask_list, dim=0)
            hsic_loss_avg = hsic_loss_total / max(num_graphs, 1)

        # ✅ 单样本分支
        else:
            g_data, center_local, node_names = _to_pyg_data(graph_data)
            if g_data is None:
                raise ValueError("❌ 单样本图数据为空。")

            g_res = self.gcn.forward(
                g_data.x.to(self.device),
                g_data.edge_index.to(self.device),
                batch_idx=None,
                return_all=True
            )

            emb_original = g_res["emb_before"]  # ✅ 修改
            emb_masked = g_res["emb_after"]  # ✅ 修改
            edge_mask_raw = g_res["edge_mask"]  # ✅ 修改

            center_emb_ori = emb_original[center_local].unsqueeze(0)
            center_emb_mask = emb_masked[center_local].unsqueeze(0)
            hsic_loss_avg = self.gcn.compute_hsic_loss(center_emb_ori, center_emb_mask)
            graph_emb = self.gcn.llm_projector(center_emb_mask)

            ei = g_data.edge_index.to(self.device)
            src, dst = ei[0], ei[1]
            num_nodes = g_data.x.size(0)
            friends_order = [i for i in range(num_nodes) if i != center_local]

            weights_in_order = []
            for i in friends_order:
                idx_0i = torch.where((src == center_local) & (dst == i))[0]
                idx_i0 = torch.where((src == i) & (dst == center_local))[0]
                ws = []
                if idx_0i.numel() > 0:
                    ws.extend(edge_mask_raw[idx_0i].tolist())
                if idx_i0.numel() > 0:
                    ws.extend(edge_mask_raw[idx_i0].tolist())
                avg_w = float(sum(ws) / len(ws)) if len(ws) > 0 else 0.0
                weights_in_order.append(avg_w)

            if len(weights_in_order) == 0:
                pad_weights = [0.0] * max_friend_len
            else:
                pad_weights = (weights_in_order * ((max_friend_len // len(weights_in_order)) + 1))[:max_friend_len]

            edge_mask = torch.tensor(pad_weights, dtype=torch.float32, device=self.device).unsqueeze(0)



        # === Tokenize ===
        all_input_ids, all_attention_masks, all_labels = [], [], []

        # ✅ 移除 tqdm，避免 DeepSpeed 冲突
        for idx, text in enumerate(inp_str):
            if not text:
                continue

            # === 编码输入 ===
            inputs = self.llm_tokenizer(
                text=text,
                max_length=max_length,
                truncation=True,
                padding=False,
                return_tensors=None,
            )

            # === 仅在训练时才编码目标 ===
            if training:
                out_text = out_str[idx] if isinstance(out_str, list) else out_str
                targets = self.llm_tokenizer(
                    text=out_text,  # ✅ 显式指定 text 参数，兼容新版本transformers
                    max_length=max_length,
                    truncation=True,
                    padding=False,
                    return_tensors=None,
                )

                total_max_length = max_length + 2048 + 1
                input_id = inputs["input_ids"] + targets["input_ids"] + [self.llm_tokenizer.eos_token_id]
                attention_mask_list = inputs["attention_mask"] + targets["attention_mask"] + [1]
                labels_list = [-100] * len(inputs["input_ids"]) + targets["input_ids"] + [
                    self.llm_tokenizer.eos_token_id]
                max_len = total_max_length
            else:
                # === 推理模式：不需要 targets ===
                input_id = inputs["input_ids"]
                attention_mask_list = inputs["attention_mask"]
                labels_list = [-100] * len(input_id)
                max_len = max_length

            # === 统一padding / 截断 ===
            pad_len = max_len - len(input_id)
            if pad_len > 0:
                input_id = [self.llm_tokenizer.pad_token_id] * pad_len + input_id
                attention_mask_list = [0] * pad_len + attention_mask_list
                labels_list = [-100] * pad_len + labels_list
            elif pad_len < 0:
                input_id = input_id[-max_len:]
                attention_mask_list = attention_mask_list[-max_len:]
                labels_list = labels_list[-max_len:]

            all_input_ids.append(input_id)
            all_attention_masks.append(attention_mask_list)
            all_labels.append(labels_list)

        input_id = torch.tensor(all_input_ids, dtype=torch.long, device=self.device)
        attention_mask = torch.tensor(all_attention_masks, dtype=torch.long, device=self.device)
        labels = torch.tensor(all_labels, dtype=torch.long, device=self.device)

        inputs_embs = self.model.get_input_embeddings()(input_id)


        # === SAE ===
        all_sparse_emb, all_recon_emb = self.sae(embeddings)

        user_emb = all_sparse_emb[:, 0:1, :]
        friend_emb = all_sparse_emb[:, 1:, :]

        # === Graph embedding ===
        if graph_emb is not None:
            # === 临时安全修复：避免 bfloat16 小 batch Linear 崩溃 ===
            orig_dtype = graph_emb.dtype
            graph_emb = graph_emb.to(torch.float32)  # ✅ 强制转 float32，绕过 CUDA bug
            self.align_mlp_graph = self.align_mlp_graph.to(torch.float32)  # ✅ 保证权重同类型

            graph_emb = self.align_mlp_graph(graph_emb)  # 这里不会再炸
            graph_emb = graph_emb.to(orig_dtype)  # ✅ 转回原 dtype（bfloat16），以兼容后续部分
            graph_emb = graph_emb.unsqueeze(1)

        # === 替换特殊 token ===
        graph_token_id = self.llm_tokenizer.convert_tokens_to_ids("[SOCIAL_GRAPH_TOKEN]")
        user_token_id = self.llm_tokenizer.convert_tokens_to_ids("[USER_TOKEN_0]")
        friend_token_ids = self.llm_tokenizer.convert_tokens_to_ids(
            [f"[FRIEND_TOKEN_{i}]" for i in range(MAX_FRIEND_LEN)])

        if graph_emb is not None:

            graph_emb = graph_emb.to(inputs_embs.dtype)
            for bidx in range(inputs_embs.shape[0]):
                mask = (input_id[bidx] == graph_token_id)
                if mask.any():
                    inputs_embs[bidx][mask] = graph_emb[bidx, 0, :]

        user_emb = self.align_mlp_user.to(torch.float32)(user_emb.to(torch.float32))
        user_emb = user_emb.to(inputs_embs.dtype)

        for bidx in range(inputs_embs.shape[0]):
            mask = (input_id[bidx] == user_token_id)
            if mask.any():
                inputs_embs[bidx][mask] = user_emb[bidx, 0, :]

        friend_emb = self.align_mlp_user.to(torch.float32)(friend_emb.to(torch.float32))
        friend_emb = friend_emb.to(inputs_embs.dtype)
        for bidx in range(inputs_embs.shape[0]):
            for i in range(MAX_FRIEND_LEN):
                mask = (input_id[bidx] == friend_token_ids[i])
                if mask.any():
                    inputs_embs[bidx][mask] = friend_emb[bidx, i, :]

        # === LLM Forward ===
        outputs = self.generate(
            inputs_embeds=inputs_embs,
            attention_mask=attention_mask,
            do_sample=True,
            use_cache=True,
            **kwargs,
        )

        text = self.llm_tokenizer.batch_decode(outputs, skip_special_tokens=True)

        return text

    @classmethod
    def from_pretrained(
            cls,
            pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
            *model_args,
            config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
            cache_dir: Optional[Union[str, os.PathLike]] = None,
            ignore_mismatched_sizes: bool = False,
            force_download: bool = False,
            local_files_only: bool = False,
            token: Optional[Union[str, bool]] = None,
            revision: str = "main",
            use_safetensors: bool = None,
            training: bool = False,
            tokenizer: Optional[AutoTokenizer] = None,
            **kwargs
    ):
        model = super().from_pretrained(
            pretrained_model_name_or_path,
            *model_args,
            config=config,
            cache_dir=cache_dir,
            ignore_mismatched_sizes=ignore_mismatched_sizes,
            force_download=force_download,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            use_safetensors=use_safetensors,
            **kwargs
        )
        model.llm_tokenizer = tokenizer
        if training:
            for name, param in model.named_parameters():
                if not any(k in name for k in ["sae", "align_mlp_user", "align_mlp_friend", "align_mlp_graph", "gcn"]):
                    param.requires_grad = False
        return model

    def loss_function(self, logits, labels, vocab_size, **kwargs):
        if labels is None:
            return torch.tensor(0.0, device=logits.device)

        # ✅ 防止维度不匹配
        if logits.size(1) != labels.size(1):
            min_len = min(logits.size(1), labels.size(1))
            logits = logits[:, :min_len, :]
            labels = labels[:, :min_len]

        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        loss = F.cross_entropy(
            shift_logits.view(-1, vocab_size),
            shift_labels.view(-1),
            ignore_index=-100,
            reduction="mean",
        )

        return loss