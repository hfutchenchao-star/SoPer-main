import os
import torch
import json
import evaluate
import numpy as np
from tqdm import tqdm
from datasets import load_from_disk
from bert_score import score as bert_score
from models.model_eval import DEPModel
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from models.gcn_gib import SocialGCN_GBSR
from torch_geometric.data import Data
import argparse
import re
# =====================================================
# âœ… å‚æ•°è®¾ç½®ï¼ˆå«è·¯å¾„é…ç½®ï¼‰
# =====================================================
parser = argparse.ArgumentParser()
parser.add_argument("--mode", choices=["infer", "eval"], default="eval", required=True, help="è¿è¡Œæ¨¡å¼ï¼šinfer æˆ– eval")
parser.add_argument("--data_dir", type=str, default="/root/autodl-tmp/my_project/data/processed_test",
                    help="æµ‹è¯•æ•°æ®è·¯å¾„")
parser.add_argument("--output_dir", type=str, default="output", help="è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„")
parser.add_argument("--llm_backbone", type=str,
                    default="/root/autodl-tmp/my_project/model_point/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28",
                    help="ä¸»å¹²æ¨¡å‹ï¼ˆæœªè®­ç»ƒçš„LLMï¼‰è·¯å¾„")
parser.add_argument("--ckpt_dir", type=str,
                    default="/root/autodl-tmp/my_project/output/checkpoint-1125",
                    help="è®­ç»ƒåä¿å­˜çš„checkpointæ–‡ä»¶å¤¹")
args = parser.parse_args()

args.ckpt_path = os.path.join(args.ckpt_dir, "trainable_modules.pt")
os.makedirs(args.output_dir, exist_ok=True)

# =====================================================
# âœ… å…¬å…±å‡½æ•°
# =====================================================
def postprocess_output(text):
    text = text.strip()
    text = text.replace("\r", " ").replace("\n", " ")
    text = " ".join(text.split())     # å»æ‰è¿ç»­ç©ºæ ¼
    return text
# =====================================================
# âœ… åŠ è½½ tokenizer
# =====================================================
print("ğŸ”§ Loading tokenizer ...")
tokenizer = AutoTokenizer.from_pretrained(args.ckpt_dir, trust_remote_code=True)

# =====================================================
# âœ… åŠ è½½ HuggingFace ä¸»å¹² LLM + DEP æ¨¡å‹
# =====================================================
print(f"ğŸ”§ Loading base model from: {args.llm_backbone}")
personal_model = DEPModel.from_pretrained(
    pretrained_model_name_or_path=args.llm_backbone,
    tokenizer=tokenizer,
    torch_dtype=torch.float16, 
    attn_implementation="flash_attention_2",
    training=False
).to("cuda")

# =====================================================
# âœ… åŠ è½½è®­ç»ƒåçš„å¯å­¦ä¹ å‚æ•°
# =====================================================
if os.path.exists(args.ckpt_path):
    print(f"ğŸ”„ Loading trained parameters from {args.ckpt_path} ...")
    state_dict = torch.load(args.ckpt_path, map_location="cpu")

    # âœ… è‡ªåŠ¨è½¬æ¢ä¸ºæ¨¡å‹ dtypeï¼ˆå¦‚ float16ï¼‰
    model_dtype = next(personal_model.parameters()).dtype
    for k, v in state_dict.items():
        if torch.is_tensor(v):
            state_dict[k] = v.to(dtype=model_dtype)

    missing, unexpected = personal_model.load_state_dict(state_dict, strict=False)
    print(f"âœ… å·²åŠ è½½è®­ç»ƒåå‚æ•°: {len(state_dict)} tensors (cast to {model_dtype})")
    if missing:
        print(f"âš ï¸ Missing keys: {missing[:5]} ...")
    if unexpected:
        print(f"âš ï¸ Unexpected keys: {unexpected[:5]} ...")
else:
    print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒåå‚æ•°æ–‡ä»¶: {args.ckpt_path}")

personal_model.eval()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯å¼€å§‹æ¨ç†æˆ–è¯„ä¼°ï¼")


# =====================================================
# âœ… åŠ è½½æµ‹è¯•æ•°æ®
# =====================================================
print(f"ğŸ“¦ Loading dataset from {args.data_dir}")
dataset = load_from_disk(args.data_dir)
print(f"âœ… Loaded {len(dataset)} samples")

# =====================================================
# âœ… è‡ªå®šä¹‰ collate_fnï¼ˆå’Œè®­ç»ƒä¿æŒä¸€è‡´ï¼‰
# =====================================================
def custom_collate_fn(batch):
    collated = {}
    collated["graph_data"] = [b.get("graph_data", None) for b in batch]
    if "embeddings" in batch[0]:
        emb_list = []
        for b in batch:
            emb = b["embeddings"]
            if isinstance(emb, list):
                emb = torch.tensor(emb, dtype=torch.float32)
            elif isinstance(emb, torch.Tensor):
                emb = emb.to(dtype=torch.float32)
            else:
                raise TypeError(f"âŒ embeddings type not supported: {type(emb)}")
            emb_list.append(emb)
        collated["embeddings"] = torch.stack(emb_list, dim=0)

    keys = set().union(*[b.keys() for b in batch])
    keys.discard("graph_data")
    keys.discard("embeddings")
    for k in keys:
        v0 = batch[0][k]
        if isinstance(v0, torch.Tensor):
            collated[k] = torch.stack([b[k] for b in batch], dim=0)
        else:
            collated[k] = [b[k] for b in batch]
    return collated

# =====================================================
# âœ… æ¨¡å¼1ï¼šæ¨ç† infer
# =====================================================
if args.mode == "infer":
    dataloader = DataLoader(dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn)
    predictions = []

    print("ğŸš€ Start inference ...")
    with torch.no_grad():
        for batch in tqdm(dataloader):
            graph_data = batch.get("graph_data", None)
            embeddings = batch.get("embeddings", None)
            inp_str = batch.get("inp_str", None)

            texts = personal_model.generate_text(
                graph_data=graph_data,
                embeddings=embeddings,
                inp_str=inp_str,
                max_new_tokens=2048,
                temperature=0.7,
                top_p=0.9,
                training=False,
            )

            predictions.extend(texts)

    # === ä¿å­˜é¢„æµ‹ç»“æœ ===
    output_path = os.path.join(args.output_dir, "predictions-justhavefriendsreviewandweight.txt")
    print(f"ğŸ’¾ Saving predictions to {output_path}")

    with open(output_path, "w", encoding="utf-8") as f:
        for pred in predictions:

            f.write(pred.strip() + "\n---------------------------------\n")

    print("âœ… Saved all predictions!")

# =====================================================
# âœ… æ¨¡å¼2ï¼šè¯„ä¼° eval
# =====================================================
elif args.mode == "eval":
    #predictions_path = os.path.join(args.output_dir, "predictions.txt")
    predictions_path = "/root/autodl-tmp/my_project/output/predictions-justhavefriendsreviewandweight.txt"

    if not os.path.exists(predictions_path):
        raise FileNotFoundError(f"âŒ {predictions_path} not found. è¯·å…ˆè¿è¡Œ `--mode infer` ç”Ÿæˆé¢„æµ‹ç»“æœã€‚")

    print(f"ğŸ“– Loading predictions from {predictions_path}")
    with open(predictions_path, "r", encoding="utf-8") as f:
        blocks = f.read().strip().split("---------------------------------")
        predictions = [b.strip() for b in blocks if b.strip()]


    references = list(dataset["out_str"])


    print("ğŸ“Š Evaluating results ...")
    bleu_metric = evaluate.load("sacrebleu")
    rouge_metric = evaluate.load("rouge")
    meteor_metric = evaluate.load("meteor")

    result_bleu = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])
    result_rouge = rouge_metric.compute(predictions=predictions, references=references)
    result_meteor = meteor_metric.compute(predictions=predictions, references=references)

    P, R, F1 = bert_score(
        predictions, references, model_type="allenai/led-base-16384", lang="en", verbose=False
    )

    result = {
        "rouge-1": float(result_rouge["rouge1"]),
        "rouge-L": float(result_rouge["rougeL"]),
        "meteor": float(result_meteor["meteor"]),
        "bleu": float(result_bleu["score"]),
        "bertscore": float(F1.mean()),
    }

    print("\nğŸ“ˆ Evaluation Results:")
    for k, v in result.items():
        print(f"{k:<10}: {v:.4f}")

    # === ä¿å­˜ç»“æœåˆ° JSON ===
    result_path = os.path.join(args.output_dir, "eval_results-justhavefriendsreviewandweight.json")
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=4)
    print(f"ğŸ’¾ Results saved to {result_path}")
